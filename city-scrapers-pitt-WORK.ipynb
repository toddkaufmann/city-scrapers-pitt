{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%matplotlib ipympl   - now in effect\n",
      "---- Boilerplate v.2019-01-09.02 loaded,  Current notebook time:\n",
      "2019-03-08T23:08:23+00:00\n",
      "CPython 3.6.5\n",
      "IPython 6.5.0\n",
      "---- loaded imports:  (?? - try  %watermark -iv)\n",
      "\n",
      "Created `%ts` as an alias for `%watermark -i`.\n"
     ]
    }
   ],
   "source": [
    "%run ../boilerplate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.5\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08T23:08:35+00:00\n"
     ]
    }
   ],
   "source": [
    "%ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following from:\n",
    "\n",
    "https://cityscrapers.org/docs/development/#spider-setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependecies\n",
    "\n",
    "No requirements.txt !  \n",
    "```\n",
    "requests = \"*\"\n",
    "twisted = \">=18.9.0\"\n",
    "scrapy = \"*\"\n",
    "scrapy-sentry = \"*\"\n",
    "legistar = {git = \"https://github.com/opencivicdata/python-legistar-scraper\"}\n",
    "city-scrapers-core = {extras = [\"aws\"],version = \"*\"}\n",
    "python-dateutil = \"*\"\n",
    "```\n",
    "\n",
    "... found ` pipenv-to-requirements`, installed / ran it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/city-scrapers-pitt\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting city-scrapers-core (from -r requirements.txt (line 13))\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/85/1d3dc82c401181104de7bbf9083ddcafa6ae7d62d09c324a63d83ff97d74/city_scrapers_core-0.2.3-py3-none-any.whl\n",
      "Collecting legistar (from -r requirements.txt (line 14))\n",
      "\u001b[31m  Could not find a version that satisfies the requirement legistar (from -r requirements.txt (line 14)) (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for legistar (from -r requirements.txt (line 14))\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/opencivicdata/python-legistar-scraper\n",
      "  Cloning https://github.com/opencivicdata/python-legistar-scraper to /tmp/pip-zvrrrnbu-build\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from legistar==0.0.1)\n",
      "Collecting lxml (from legistar==0.0.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/8a/5e066949f2b40caac32c7b2a77da63ad304b5fbe869036cc3fe4a198f724/lxml-4.3.3-cp36-cp36m-manylinux1_x86_64.whl (5.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.7MB 80kB/s \n",
      "\u001b[?25hRequirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from legistar==0.0.1)\n",
      "Collecting icalendar (from legistar==0.0.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/00/af65fe57330c5b087ee497d3e412b03432a1823a46bac6900f956b9393a1/icalendar-4.0.3-py2.py3-none-any.whl (73kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 2.9MB/s \n",
      "\u001b[?25hCollecting scrapelib (from legistar==0.0.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/85/ca29e44748abe598daffe1a6dad8a175f3acff57fe09daab040ab0bf604a/scrapelib-1.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->legistar==0.0.1)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->legistar==0.0.1)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->legistar==0.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->legistar==0.0.1)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.6/site-packages (from icalendar->legistar==0.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil->icalendar->legistar==0.0.1)\n",
      "Installing collected packages: lxml, icalendar, scrapelib, legistar\n",
      "  Running setup.py install for legistar ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed icalendar-4.0.3 legistar-0.0.1 lxml-4.3.3 scrapelib-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/opencivicdata/python-legistar-scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting city-scrapers-core (from -r requirements.txt (line 13))\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/85/1d3dc82c401181104de7bbf9083ddcafa6ae7d62d09c324a63d83ff97d74/city_scrapers_core-0.2.3-py3-none-any.whl\n",
      "Requirement already satisfied: legistar in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 14))\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 15))\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 16))\n",
      "Collecting scrapy (from -r requirements.txt (line 17))\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/45/414e87ac8209d537c91575538c5307c20217a6943f555e0ee39f6db4bb0f/Scrapy-1.6.0-py2.py3-none-any.whl (231kB)\n",
      "\u001b[K    100% |████████████████████████████████| 235kB 1.5MB/s \n",
      "\u001b[?25hCollecting scrapy-sentry (from -r requirements.txt (line 18))\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/commands/install.py\", line 335, in run\n",
      "    wb.build(autobuilding=True)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/wheel.py\", line 756, in build\n",
      "    self.requirement_set.prepare_files(self.finder)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/req/req_set.py\", line 380, in prepare_files\n",
      "    ignore_dependencies=self.ignore_dependencies))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/req/req_set.py\", line 554, in _prepare_file\n",
      "    require_hashes\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/req/req_install.py\", line 281, in populate_link\n",
      "    self.link = self._wheel_cache.cached_wheel(self.link, self.name)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/wheel.py\", line 68, in cached_wheel\n",
      "    self._cache_dir, link, self._format_control, package_name)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/wheel.py\", line 129, in cached_wheel\n",
      "    wheel_names = os.listdir(root)\n",
      "PermissionError: [Errno 13] Permission denied: '/home/jovyan/.cache/pip/wheels/3b/6e/87/b7ca172f9cffa93aeca8344e081117887bf6aa6c4ff5d1d56e'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy-sentry\n",
      "  Downloading https://files.pythonhosted.org/packages/f3/c2/9471002c29ea2996e94d321502276dceed1dd74226eff37576480a583edb/scrapy-sentry-0.9.0.tar.gz\n",
      "Collecting Scrapy>=1.4.0 (from scrapy-sentry)\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/45/414e87ac8209d537c91575538c5307c20217a6943f555e0ee39f6db4bb0f/Scrapy-1.6.0-py2.py3-none-any.whl (231kB)\n",
      "\u001b[K    100% |████████████████████████████████| 235kB 1.7MB/s \n",
      "\u001b[?25hCollecting raven>=6.3.0 (from scrapy-sentry)\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/8e/62e26a88c0a1bbae677200baf0767c1022321a6555634f8129e6d55c5ddc/raven-6.10.0-py2.py3-none-any.whl (284kB)\n",
      "\u001b[K    100% |████████████████████████████████| 286kB 1.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from scrapy-sentry)\n",
      "Collecting cssselect>=0.9 (from Scrapy>=1.4.0->scrapy-sentry)\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/44/25b7283e50585f0b4156960691d951b05d061abf4a714078393e51929b30/cssselect-1.0.3-py2.py3-none-any.whl\n",
      "Collecting parsel>=1.5 (from Scrapy>=1.4.0->scrapy-sentry)\n",
      "  Downloading https://files.pythonhosted.org/packages/96/69/d1d5dba5e4fecd41ffd71345863ed36a45975812c06ba77798fc15db6a64/parsel-1.5.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: Twisted>=13.1.0 in /opt/conda/lib/python3.6/site-packages (from Scrapy>=1.4.0->scrapy-sentry)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.6/site-packages (from Scrapy>=1.4.0->scrapy-sentry)\n",
      "Requirement already satisfied: service-identity in /opt/conda/lib/python3.6/site-packages (from Scrapy>=1.4.0->scrapy-sentry)\n",
      "Collecting PyDispatcher>=2.0.5 (from Scrapy>=1.4.0->scrapy-sentry)\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
      "Requirement already satisfied: pyOpenSSL in /opt/conda/lib/python3.6/site-packages (from Scrapy>=1.4.0->scrapy-sentry)\n",
      "Collecting w3lib>=1.17.0 (from Scrapy>=1.4.0->scrapy-sentry)\n",
      "  Downloading https://files.pythonhosted.org/packages/81/43/9dcf92a77f5f0afe4f4df2407d7289eea01368a08b64bda00dd318ca62a6/w3lib-1.20.0-py2.py3-none-any.whl\n",
      "Collecting queuelib (from Scrapy>=1.4.0->scrapy-sentry)\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in /opt/conda/lib/python3.6/site-packages (from Twisted>=13.1.0->Scrapy>=1.4.0->scrapy-sentry)\n",
      "Requirement already satisfied: constantly>=15.1 in /opt/conda/lib/python3.6/site-packages (from Twisted>=13.1.0->Scrapy>=1.4.0->scrapy-sentry)\n",
      "Requirement already satisfied: incremental>=16.10.1 in /opt/conda/lib/python3.6/site-packages (from Twisted>=13.1.0->Scrapy>=1.4.0->scrapy-sentry)\n",
      "Collecting Automat>=0.3.0 (from Twisted>=13.1.0->Scrapy>=1.4.0->scrapy-sentry)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/86/14c16bb98a5a3542ed8fed5d74fb064a902de3bdd98d6584b34553353c45/Automat-0.7.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /opt/conda/lib/python3.6/site-packages (from Twisted>=13.1.0->Scrapy>=1.4.0->scrapy-sentry)\n",
      "Collecting PyHamcrest>=1.9.0 (from Twisted>=13.1.0->Scrapy>=1.4.0->scrapy-sentry)\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/d5/d37fd731b7d0e91afcc84577edeccf4638b4f9b82f5ffe2f8b62e2ddc609/PyHamcrest-1.9.0-py2.py3-none-any.whl (52kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 2.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from Twisted>=13.1.0->Scrapy>=1.4.0->scrapy-sentry)\n",
      "Requirement already satisfied: pyasn1 in /opt/conda/lib/python3.6/site-packages (from service-identity->Scrapy>=1.4.0->scrapy-sentry)\n",
      "Requirement already satisfied: pyasn1-modules in /opt/conda/lib/python3.6/site-packages (from service-identity->Scrapy>=1.4.0->scrapy-sentry)\n",
      "Requirement already satisfied: cryptography>=2.2.1 in /opt/conda/lib/python3.6/site-packages (from pyOpenSSL->Scrapy>=1.4.0->scrapy-sentry)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from zope.interface>=4.4.2->Twisted>=13.1.0->Scrapy>=1.4.0->scrapy-sentry)\n",
      "Requirement already satisfied: idna>=2.1 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.2.1->pyOpenSSL->Scrapy>=1.4.0->scrapy-sentry)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.2.1->pyOpenSSL->Scrapy>=1.4.0->scrapy-sentry)\n",
      "Requirement already satisfied: cffi>=1.7 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.2.1->pyOpenSSL->Scrapy>=1.4.0->scrapy-sentry)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.7->cryptography>=2.2.1->pyOpenSSL->Scrapy>=1.4.0->scrapy-sentry)\n",
      "Building wheels for collected packages: scrapy-sentry, PyDispatcher\n",
      "  Running setup.py bdist_wheel for scrapy-sentry ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/3b/6e/87/b7ca172f9cffa93aeca8344e081117887bf6aa6c4ff5d1d56e\n",
      "  Running setup.py bdist_wheel for PyDispatcher ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
      "Successfully built scrapy-sentry PyDispatcher\n",
      "Installing collected packages: cssselect, w3lib, parsel, PyDispatcher, queuelib, Scrapy, raven, scrapy-sentry, Automat, PyHamcrest\n",
      "  Found existing installation: Automat 0.0.0\n",
      "    Uninstalling Automat-0.0.0:\n",
      "      Successfully uninstalled Automat-0.0.0\n",
      "Successfully installed Automat-0.7.0 PyDispatcher-2.0.5 PyHamcrest-1.9.0 Scrapy-1.6.0 cssselect-1.0.3 parsel-1.5.1 queuelib-1.5.0 raven-6.10.0 scrapy-sentry-0.9.0 w3lib-1.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy-sentry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ?? for some reason it didn't work the first time .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting city-scrapers-core (from -r requirements.txt (line 13))\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/85/1d3dc82c401181104de7bbf9083ddcafa6ae7d62d09c324a63d83ff97d74/city_scrapers_core-0.2.3-py3-none-any.whl\n",
      "Requirement already satisfied: legistar in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 14))\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 15))\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 16))\n",
      "Requirement already satisfied: scrapy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 17))\n",
      "Requirement already satisfied: scrapy-sentry in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 18))\n",
      "Collecting twisted>=18.9.0 (from -r requirements.txt (line 19))\n",
      "  Downloading https://files.pythonhosted.org/packages/5d/0e/a72d85a55761c2c3ff1cb968143a2fd5f360220779ed90e0fadf4106d4f2/Twisted-18.9.0.tar.bz2 (3.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.1MB 184kB/s \n",
      "\u001b[?25hRequirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from city-scrapers-core->-r requirements.txt (line 13))\n",
      "Collecting jsonschema>=3.0.0a5 (from city-scrapers-core->-r requirements.txt (line 13))\n",
      "  Downloading https://files.pythonhosted.org/packages/aa/69/df679dfbdd051568b53c38ec8152a3ab6bc533434fc7ed11ab034bf5e82f/jsonschema-3.0.1-py2.py3-none-any.whl (54kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 2.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: lxml in /opt/conda/lib/python3.6/site-packages (from legistar->-r requirements.txt (line 14))\n",
      "Requirement already satisfied: icalendar in /opt/conda/lib/python3.6/site-packages (from legistar->-r requirements.txt (line 14))\n",
      "Requirement already satisfied: scrapelib in /opt/conda/lib/python3.6/site-packages (from legistar->-r requirements.txt (line 14))\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil->-r requirements.txt (line 15))\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: pyOpenSSL in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: service-identity in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: parsel>=1.5 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: cssselect>=0.9 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: queuelib in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: raven>=6.3.0 in /opt/conda/lib/python3.6/site-packages (from scrapy-sentry->-r requirements.txt (line 18))\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: constantly>=15.1 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: incremental>=16.10.1 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: Automat>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: PyHamcrest>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Collecting pyrsistent>=0.14.0 (from jsonschema>=3.0.0a5->city-scrapers-core->-r requirements.txt (line 13))\n",
      "  Downloading https://files.pythonhosted.org/packages/8c/46/4e93ab8a379d7efe93f20a0fb8a27bdfe88942cc954ab0210c3164e783e0/pyrsistent-0.14.11.tar.gz (104kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 3.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from jsonschema>=3.0.0a5->city-scrapers-core->-r requirements.txt (line 13))\n",
      "Requirement already satisfied: cryptography>=2.2.1 in /opt/conda/lib/python3.6/site-packages (from pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: pyasn1 in /opt/conda/lib/python3.6/site-packages (from service-identity->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: pyasn1-modules in /opt/conda/lib/python3.6/site-packages (from service-identity->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: cffi>=1.7 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.7->cryptography>=2.2.1->pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Building wheels for collected packages: twisted, pyrsistent\n",
      "  Running setup.py bdist_wheel for twisted ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/57/2e/89/11ba83bc08ac30a5e3a6005f0310c78d231b96a270def88ca0\n",
      "  Running setup.py bdist_wheel for pyrsistent ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/83/59/9a/a037b9b3c3e93d9275ea0aff9d6064400f372879dfdab01afe\n",
      "Successfully built twisted pyrsistent\n",
      "Installing collected packages: pyrsistent, jsonschema, city-scrapers-core, twisted\n",
      "  Found existing installation: jsonschema 2.6.0\n",
      "    Uninstalling jsonschema-2.6.0:\n",
      "      Successfully uninstalled jsonschema-2.6.0\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/commands/install.py\", line 342, in run\n",
      "    prefix=options.prefix_path,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/req/req_set.py\", line 784, in install\n",
      "    **kwargs\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/req/req_install.py\", line 851, in install\n",
      "    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/req/req_install.py\", line 1064, in move_wheel_files\n",
      "    isolated=self.isolated,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/wheel.py\", line 352, in move_wheel_files\n",
      "    clobber(source, lib_dir, True)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/wheel.py\", line 330, in clobber\n",
      "    shutil.copyfile(srcfile, destfile)\n",
      "  File \"/opt/conda/lib/python3.6/shutil.py\", line 121, in copyfile\n",
      "    with open(dst, 'wb') as fdst:\n",
      "PermissionError: [Errno 13] Permission denied: '/opt/conda/lib/python3.6/site-packages/tests/test_pipelines.py'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from jsonschema)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema)\n"
     ]
    }
   ],
   "source": [
    "# why ?\n",
    "!pip install jsonschema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting city-scrapers-core (from -r requirements.txt (line 13))\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/85/1d3dc82c401181104de7bbf9083ddcafa6ae7d62d09c324a63d83ff97d74/city_scrapers_core-0.2.3-py3-none-any.whl\n",
      "Requirement already satisfied: legistar in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 14))\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 15))\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 16))\n",
      "Requirement already satisfied: scrapy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 17))\n",
      "Requirement already satisfied: scrapy-sentry in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 18))\n",
      "Collecting twisted>=18.9.0 (from -r requirements.txt (line 19))\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from city-scrapers-core->-r requirements.txt (line 13))\n",
      "Requirement already satisfied: jsonschema>=3.0.0a5 in /opt/conda/lib/python3.6/site-packages (from city-scrapers-core->-r requirements.txt (line 13))\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.6/site-packages (from legistar->-r requirements.txt (line 14))\n",
      "Requirement already satisfied: icalendar in /opt/conda/lib/python3.6/site-packages (from legistar->-r requirements.txt (line 14))\n",
      "Requirement already satisfied: scrapelib in /opt/conda/lib/python3.6/site-packages (from legistar->-r requirements.txt (line 14))\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil->-r requirements.txt (line 15))\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: pyOpenSSL in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: cssselect>=0.9 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: service-identity in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: parsel>=1.5 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: queuelib in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: raven>=6.3.0 in /opt/conda/lib/python3.6/site-packages (from scrapy-sentry->-r requirements.txt (line 18))\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: Automat>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: constantly>=15.1 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: PyHamcrest>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: incremental>=16.10.1 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema>=3.0.0a5->city-scrapers-core->-r requirements.txt (line 13))\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from jsonschema>=3.0.0a5->city-scrapers-core->-r requirements.txt (line 13))\n",
      "Requirement already satisfied: cryptography>=2.2.1 in /opt/conda/lib/python3.6/site-packages (from pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: pyasn1 in /opt/conda/lib/python3.6/site-packages (from service-identity->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: pyasn1-modules in /opt/conda/lib/python3.6/site-packages (from service-identity->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: cffi>=1.7 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.7->cryptography>=2.2.1->pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Installing collected packages: city-scrapers-core, twisted\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/commands/install.py\", line 342, in run\n",
      "    prefix=options.prefix_path,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/req/req_set.py\", line 784, in install\n",
      "    **kwargs\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/req/req_install.py\", line 851, in install\n",
      "    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/req/req_install.py\", line 1064, in move_wheel_files\n",
      "    isolated=self.isolated,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/wheel.py\", line 352, in move_wheel_files\n",
      "    clobber(source, lib_dir, True)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/wheel.py\", line 330, in clobber\n",
      "    shutil.copyfile(srcfile, destfile)\n",
      "  File \"/opt/conda/lib/python3.6/shutil.py\", line 121, in copyfile\n",
      "    with open(dst, 'wb') as fdst:\n",
      "PermissionError: [Errno 13] Permission denied: '/opt/conda/lib/python3.6/site-packages/tests/test_pipelines.py'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting city-scrapers-core (from -r requirements.txt (line 13))\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/85/1d3dc82c401181104de7bbf9083ddcafa6ae7d62d09c324a63d83ff97d74/city_scrapers_core-0.2.3-py3-none-any.whl\n",
      "Requirement already satisfied: legistar in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 14))\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 15))\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 16))\n",
      "Requirement already satisfied: scrapy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 17))\n",
      "Requirement already satisfied: scrapy-sentry in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 18))\n",
      "Collecting twisted>=18.9.0 (from -r requirements.txt (line 19))\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from city-scrapers-core->-r requirements.txt (line 13))\n",
      "Requirement already satisfied: jsonschema>=3.0.0a5 in /opt/conda/lib/python3.6/site-packages (from city-scrapers-core->-r requirements.txt (line 13))\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.6/site-packages (from legistar->-r requirements.txt (line 14))\n",
      "Requirement already satisfied: icalendar in /opt/conda/lib/python3.6/site-packages (from legistar->-r requirements.txt (line 14))\n",
      "Requirement already satisfied: scrapelib in /opt/conda/lib/python3.6/site-packages (from legistar->-r requirements.txt (line 14))\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil->-r requirements.txt (line 15))\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: service-identity in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: cssselect>=0.9 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: parsel>=1.5 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: pyOpenSSL in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: queuelib in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: raven>=6.3.0 in /opt/conda/lib/python3.6/site-packages (from scrapy-sentry->-r requirements.txt (line 18))\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: constantly>=15.1 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: PyHamcrest>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: Automat>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: incremental>=16.10.1 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema>=3.0.0a5->city-scrapers-core->-r requirements.txt (line 13))\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from jsonschema>=3.0.0a5->city-scrapers-core->-r requirements.txt (line 13))\n",
      "Requirement already satisfied: pyasn1 in /opt/conda/lib/python3.6/site-packages (from service-identity->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: pyasn1-modules in /opt/conda/lib/python3.6/site-packages (from service-identity->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: cryptography>=2.2.1 in /opt/conda/lib/python3.6/site-packages (from pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: cffi>=1.7 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.7->cryptography>=2.2.1->pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Installing collected packages: city-scrapers-core, twisted\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/commands/install.py\", line 342, in run\n",
      "    prefix=options.prefix_path,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/req/req_set.py\", line 784, in install\n",
      "    **kwargs\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/req/req_install.py\", line 851, in install\n",
      "    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/req/req_install.py\", line 1064, in move_wheel_files\n",
      "    isolated=self.isolated,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/wheel.py\", line 352, in move_wheel_files\n",
      "    clobber(source, lib_dir, True)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pip/wheel.py\", line 330, in clobber\n",
      "    shutil.copyfile(srcfile, destfile)\n",
      "  File \"/opt/conda/lib/python3.6/shutil.py\", line 121, in copyfile\n",
      "    with open(dst, 'wb') as fdst:\n",
      "PermissionError: [Errno 13] Permission denied: '/opt/conda/lib/python3.6/site-packages/tests/__pycache__/test_spiders.cpython-36.pyc'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# as root\n",
    "# cd /opt/conda/lib/python3.6/site-packages/tests/\n",
    "# chmod 777\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting city-scrapers-core (from -r requirements.txt (line 13))\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/85/1d3dc82c401181104de7bbf9083ddcafa6ae7d62d09c324a63d83ff97d74/city_scrapers_core-0.2.3-py3-none-any.whl\n",
      "Requirement already satisfied: legistar in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 14))\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 15))\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 16))\n",
      "Requirement already satisfied: scrapy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 17))\n",
      "Requirement already satisfied: scrapy-sentry in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 18))\n",
      "Collecting twisted>=18.9.0 (from -r requirements.txt (line 19))\n",
      "Requirement already satisfied: jsonschema>=3.0.0a5 in /opt/conda/lib/python3.6/site-packages (from city-scrapers-core->-r requirements.txt (line 13))\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from city-scrapers-core->-r requirements.txt (line 13))\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.6/site-packages (from legistar->-r requirements.txt (line 14))\n",
      "Requirement already satisfied: icalendar in /opt/conda/lib/python3.6/site-packages (from legistar->-r requirements.txt (line 14))\n",
      "Requirement already satisfied: scrapelib in /opt/conda/lib/python3.6/site-packages (from legistar->-r requirements.txt (line 14))\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil->-r requirements.txt (line 15))\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: cssselect>=0.9 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: pyOpenSSL in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: queuelib in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: service-identity in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: parsel>=1.5 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: raven>=6.3.0 in /opt/conda/lib/python3.6/site-packages (from scrapy-sentry->-r requirements.txt (line 18))\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: PyHamcrest>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: Automat>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: constantly>=15.1 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: incremental>=16.10.1 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema>=3.0.0a5->city-scrapers-core->-r requirements.txt (line 13))\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from jsonschema>=3.0.0a5->city-scrapers-core->-r requirements.txt (line 13))\n",
      "Requirement already satisfied: cryptography>=2.2.1 in /opt/conda/lib/python3.6/site-packages (from pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: pyasn1 in /opt/conda/lib/python3.6/site-packages (from service-identity->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: pyasn1-modules in /opt/conda/lib/python3.6/site-packages (from service-identity->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: cffi>=1.7 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.7->cryptography>=2.2.1->pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Installing collected packages: city-scrapers-core, twisted\n",
      "  Found existing installation: Twisted 18.7.0\n",
      "    Uninstalling Twisted-18.7.0:\n",
      "      Successfully uninstalled Twisted-18.7.0\n",
      "Successfully installed city-scrapers-core-0.2.3 twisted-18.9.0\n"
     ]
    }
   ],
   "source": [
    "# again - \n",
    "# cd /opt/conda/lib/python3.6/site-packages/tests/__pycache__\n",
    "#chmod 777 .\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest\n",
      "  Downloading https://files.pythonhosted.org/packages/c8/52/c455e718f19e4fd7126c81b4910ca2431ded6c28b97e36a1a1f5e5ef7247/pytest-4.3.1-py2.py3-none-any.whl (219kB)\n",
      "\u001b[K    100% |████████████████████████████████| 225kB 1.6MB/s \n",
      "\u001b[?25hCollecting pluggy>=0.7 (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/84/e8/4ddac125b5a0e84ea6ffc93cfccf1e7ee1924e88f53c64e98227f0af2a5f/pluggy-0.9.0-py2.py3-none-any.whl\n",
      "Collecting py>=1.5.0 (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/76/bc/394ad449851729244a97857ee14d7cba61ddb268dce3db538ba2f2ba1f0f/py-1.8.0-py2.py3-none-any.whl (83kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 4.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from pytest)\n",
      "Collecting atomicwrites>=1.0 (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/52/90/6155aa926f43f2b2a22b01be7241be3bfd1ceaf7d0b3267213e8127d41f4/atomicwrites-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from pytest)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from pytest)\n",
      "Collecting more-itertools>=4.0.0; python_version > \"2.7\" (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/73/64fb5922b745fc1daee8a2880d907d2a70d9c7bb71eea86fcb9445daab5e/more_itertools-7.0.0-py3-none-any.whl (53kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 3.7MB/s \n",
      "\u001b[?25hInstalling collected packages: pluggy, py, atomicwrites, more-itertools, pytest\n",
      "Successfully installed atomicwrites-1.3.0 more-itertools-7.0.0 pluggy-0.9.0 py-1.8.0 pytest-4.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting freezegun\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/1a/4735261786db47cfd99e99b46537ada0831667b492a663675369a1c1560d/freezegun-0.3.11-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from freezegun)\n",
      "Requirement already satisfied: python-dateutil!=2.0,>=1.0 in /opt/conda/lib/python3.6/site-packages (from freezegun)\n",
      "Installing collected packages: freezegun\n",
      "Successfully installed freezegun-0.3.11\n"
     ]
    }
   ],
   "source": [
    "# pytest wants\n",
    "!pip install freezegun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tk: install dev deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting city-scrapers-core (from -r requirements.txt (line 13))\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/85/1d3dc82c401181104de7bbf9083ddcafa6ae7d62d09c324a63d83ff97d74/city_scrapers_core-0.2.3-py3-none-any.whl\n",
      "Requirement already satisfied: legistar in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 14))\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 15))\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 16))\n",
      "Requirement already satisfied: scrapy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 17))\n",
      "Requirement already satisfied: scrapy-sentry in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 18))\n",
      "Collecting twisted>=18.9.0 (from -r requirements.txt (line 19))\n",
      "Requirement already satisfied: jsonschema>=3.0.0a5 in /opt/conda/lib/python3.6/site-packages (from city-scrapers-core->-r requirements.txt (line 13))\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from city-scrapers-core->-r requirements.txt (line 13))\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.6/site-packages (from legistar->-r requirements.txt (line 14))\n",
      "Requirement already satisfied: icalendar in /opt/conda/lib/python3.6/site-packages (from legistar->-r requirements.txt (line 14))\n",
      "Requirement already satisfied: scrapelib in /opt/conda/lib/python3.6/site-packages (from legistar->-r requirements.txt (line 14))\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil->-r requirements.txt (line 15))\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->-r requirements.txt (line 16))\n",
      "Requirement already satisfied: cssselect>=0.9 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: pyOpenSSL in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: queuelib in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: service-identity in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: parsel>=1.5 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /opt/conda/lib/python3.6/site-packages (from scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: raven>=6.3.0 in /opt/conda/lib/python3.6/site-packages (from scrapy-sentry->-r requirements.txt (line 18))\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: PyHamcrest>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: Automat>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: constantly>=15.1 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: incremental>=16.10.1 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from twisted>=18.9.0->-r requirements.txt (line 19))\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema>=3.0.0a5->city-scrapers-core->-r requirements.txt (line 13))\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from jsonschema>=3.0.0a5->city-scrapers-core->-r requirements.txt (line 13))\n",
      "Requirement already satisfied: cryptography>=2.2.1 in /opt/conda/lib/python3.6/site-packages (from pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: pyasn1 in /opt/conda/lib/python3.6/site-packages (from service-identity->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: pyasn1-modules in /opt/conda/lib/python3.6/site-packages (from service-identity->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: cffi>=1.7 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.7->cryptography>=2.2.1->pyOpenSSL->scrapy->-r requirements.txt (line 17))\n",
      "Installing collected packages: city-scrapers-core, twisted\n",
      "  Found existing installation: Twisted 18.7.0\n",
      "    Uninstalling Twisted-18.7.0:\n",
      "      Successfully uninstalled Twisted-18.7.0\n",
      "Successfully installed city-scrapers-core-0.2.3 twisted-18.9.0\n"
     ]
    }
   ],
   "source": [
    "# dev\n",
    "!pip install -r requirements-dev.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install success!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spider Setup\n",
    "\n",
    "# 1. Find a site to scrape and create an issues\n",
    "First, find an unclaimed event source within the project’s issues. Any unassigned issue is fair game. Add a comment indicating that you’re interested in the work.\n",
    "\n",
    "Save and note the issue number.\n",
    "\n",
    "## tk - issue 10 - \n",
    "URL: http://www.flypittsburgh.com/about-us/leadership\n",
    "\n",
    "TODO:meta: find links to this, find copies of meeting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a new branch\n",
    "Create a new branch in your fork\n",
    "    \n",
    "    $ git checkout -b XXXX-spider-NAMEOFAGENCY\n",
    "XXXX is the zero-padded issue number and \n",
    "NAMEOFAGENCY should be something like chi_housing. \n",
    "For example, for ticket number 53 entitled “SPIDER: Chicago Housing Authority”, create a branch named 0053-spider-chi_housing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tk: my branch: 0010-spider-alle_airport_authority\n",
    "\n",
    "`git checkout -b 0010-spider-alle_airport_authority`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create a spider\n",
    "Create a spider from our template with a spider slug, agency name, and a URL to start scraping. Inside your virtual environment following the previous examples (or prefixed by pipenv run) run:\n",
    "\n",
    "    (city-scrapers)$ scrapy genspider chi_housing \"Chicago Housing Authority\" http://www.thecha.org\n",
    "You should see some output like:\n",
    "```\n",
    "Created file: /Users/eads/Code/city-scrapers/city_scrapers/spiders/chi_housing.py\n",
    "Created file: /Users/eads/Code/city-scrapers/tests/test_chi_housing.py\n",
    "Created file: /Users/eads/Code/city-scrapers/tests/files/chi_housing.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file: /home/jovyan/work/city-scrapers-pitt/tests/files/alle_airport_authority.html\n",
      "Created file: /home/jovyan/work/city-scrapers-pitt/city_scrapers/spiders/alle_airport_authority.py\n",
      "Created file: /home/jovyan/work/city-scrapers-pitt/tests/test_alle_airport_authority.py\n"
     ]
    }
   ],
   "source": [
    "## tk: creating spider..\n",
    "!scrapy genspider alle_airport_authority \"Allegheny County Airport Authority\" http://www.flypittsburgh.com/about-us/leadership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tk: creating spider..\n",
    "\n",
    "!scrapy genspider alle_airport_authority \"Allegheny County Airport Authority\" http://www.flypittsburgh.com/about-us/leadership\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test crawling\n",
    "You now have a spider named chi_housing. To run it (admittedly, not much will happen until you start editing the scraper), run:\n",
    "\n",
    "(city-scrapers)$ scrapy crawl chi_housing\n",
    "If there are no error messages, congratulations! You have a barebones spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-30 02:28:46 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: city_scrapers)\n",
      "2019-03-30 02:28:46 [scrapy.utils.log] INFO: Versions: lxml 4.3.3.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.6.5 | packaged by conda-forge | (default, Apr  6 2018, 13:39:56) - [GCC 4.8.2 20140120 (Red Hat 4.8.2-15)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2p  14 Aug 2018), cryptography 2.2.1, Platform Linux-4.4.0-92-generic-x86_64-with-debian-buster-sid\n",
      "2019-03-30 02:28:46 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'city_scrapers', 'CLOSESPIDER_ERRORCOUNT': 5, 'COMMANDS_MODULE': 'city_scrapers_core.commands', 'COOKIES_ENABLED': False, 'NEWSPIDER_MODULE': 'city_scrapers.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['city_scrapers.spiders'], 'USER_AGENT': 'City Scrapers [development mode]. Learn more and say hello at https://www.citybureau.org/city-scrapers/'}\n",
      "2019-03-30 02:28:46 [scrapy.extensions.telnet] INFO: Telnet Password: 10a0404e4b0fad87\n",
      "2019-03-30 02:28:46 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-03-30 02:28:46 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-03-30 02:28:46 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-03-30 02:28:46 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "['city_scrapers_core.pipelines.DefaultValuesPipeline',\n",
      " 'city_scrapers_core.pipelines.MeetingPipeline']\n",
      "2019-03-30 02:28:46 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-03-30 02:28:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-03-30 02:28:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-03-30 02:28:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://www.flypittsburgh.com/robots.txt> (referer: None)\n",
      "2019-03-30 02:28:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.flypittsburgh.com/about-us/leadership> (referer: None)\n",
      "2019-03-30 02:28:47 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-03-30 02:28:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 607,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 19225,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/404': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 3, 30, 2, 28, 47, 961380),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 9,\n",
      " 'memusage/max': 58376192,\n",
      " 'memusage/startup': 58376192,\n",
      " 'response_received_count': 2,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/404': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2019, 3, 30, 2, 28, 46, 653720)}\n",
      "2019-03-30 02:28:47 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# my test..\n",
    "!scrapy crawl alle_airport_authority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run the automated tests\n",
    "We use the pytest testing framework to verify the behavior of the project’s code. To run this, simply run pytest in your project environment.\n",
    "```\n",
    "(city-scrapers)$ pytest\n",
    "Whoops! The tests for new spiders fail by default. Here’s typical output:\n",
    "\n",
    "====================================== test session starts =======================================\n",
    "platform darwin -- Python 3.6.2, pytest-3.1.3, py-1.4.34, pluggy-0.4.0\n",
    "rootdir: /Users/eads/projects/city-scrapers, inifile:\n",
    "collected 59 items\n",
    "\n",
    "tests/test_chi_housing.py F\n",
    "tests/test_idph.py .......................................................\n",
    "tests/test_tasks.py ...\n",
    "\n",
    "============================================ FAILURES ============================================\n",
    "___________________________________________ test_tests ___________________________________________\n",
    "\n",
    "    def test_tests():\n",
    "        print('Please write some tests for this spider or at least disable this one.')\n",
    ">       assert False\n",
    "E       assert False\n",
    "\n",
    "tests/test_chi_housing.py:8: AssertionError\n",
    "-------------------------------------- Captured stdout call --------------------------------------\n",
    "Please write some tests for this spider or at least disable this one.\n",
    "============================== 1 failed, 58 passed in 0.95 seconds ==============================\n",
    "```\n",
    "That’s OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOU ARE HERE <<<<<<<=========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.5, pytest-4.3.1, py-1.8.0, pluggy-0.9.0\n",
      "rootdir: /home/jovyan/work/city-scrapers-pitt, inifile: setup.cfg\n",
      "collected 36 items                                                             \u001b[0m\n",
      "\n",
      "tests/test_alle_airport_authority.py \u001b[31mF\u001b[0m\u001b[36m                                   [  2%]\u001b[0m\n",
      "tests/test_alle_county.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[36m                                  [ 38%]\u001b[0m\n",
      "tests/test_alle_port_authority.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[36m                 [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m__________________________________ test_tests __________________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_tests():\u001b[0m\n",
      "\u001b[1m        print(\"Please write some tests for this spider or at least disable this one.\")\u001b[0m\n",
      "\u001b[1m>       assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_alle_airport_authority.py\u001b[0m:27: AssertionError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "Please write some tests for this spider or at least disable this one.\n",
      "\u001b[31m\u001b[1m===================== 1 failed, 35 passed in 1.70 seconds ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# try running (non-existent) tests\n",
    "!pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-30T17:42:16+00:00\n"
     ]
    }
   ],
   "source": [
    "%ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tk: good / expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Run linting and style-checking tools\n",
    "We use flake8, isort, and yapf to check that all code is written in the proper style. \n",
    "To run these tools individually, you can run the following commands:\n",
    "```\n",
    "$ flake8\n",
    "$ isort\n",
    "$ yapf --in-place --recursive ./city_scrapers/ ./tests/\n",
    "Most text editors can be configured to fix style issues for you based off of the configuration settings in setup.cfg. \n",
    "Here’s an example for VSCode using the standard Python extension \n",
    "(which can be modified/added at .vscode/settings.json in your project directory):\n",
    "\n",
    "{\n",
    "  \"python.pythonPath\": \"${workspaceFolder}/.venv/bin/python\",\n",
    "  \"python.linting.pylintEnabled\": false,\n",
    "  \"python.linting.flake8Enabled\": true,\n",
    "  \"python.envFile\": \"${workspaceRoot}/.env\",\n",
    "  \"python.linting.flake8Args\": [\"--config\", \"${workspaceRoot}/setup.cfg\"],\n",
    "  \"python.formatting.provider\": \"yapf\",\n",
    "  \"python.formatting.yapfArgs\": [\"--style\", \"${workspaceRoot}/setup.cfg\"],\n",
    "  \"python.sortImports.path\": \"${workspaceRoot}/setup.cfg\",\n",
    "  \"editor.formatOnSave\": true,\n",
    "  \"editor.rulers\": [100]\n",
    "}\n",
    "```\n",
    "This configuration will run linting and style checks for you, and also make necessary changes automatically any time you save. Packages are available for Atom and Sublime Text as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tk: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: flake8: not found\n"
     ]
    }
   ],
   "source": [
    "!flake8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: isort: not found\n"
     ]
    }
   ],
   "source": [
    "!isort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: yapf: not found\n"
     ]
    }
   ],
   "source": [
    "!yapf --in-place --recursive ./city_scrapers/ ./tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refresher: calling / interacting w/ scrapy from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Building a spider\n",
    "## A. Write parse methods in the spider\n",
    "Open city_scrapers/spiders/chi_housing.py to work on your spider. A simple structure has been created for you to use. Let’s look at the basics.\n",
    "\n",
    "The spider should look something like this:\n",
    "\n",
    "see: https://cityscrapers.org/docs/development/#a-write-parse-methods-in-the-spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tk:  in my case, the spider is at:    <<fill in>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !! follows is from old NB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy shell -- jupyter equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-30T17:46:36+00:00\n"
     ]
    }
   ],
   "source": [
    "%ts\n",
    "# see:\n",
    "# https://stackoverflow.com/questions/26177620/how-to-execute-scrapy-shell-url-with-notebook\n",
    "\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "url = \"http://www.flypittsburgh.com/about-us/leadership\"\n",
    "\n",
    "r = requests.get( url )\n",
    "response = TextResponse(r.url, body=r.text, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200 http://www.flypittsburgh.com/about-us/leadership>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector xpath=\"descendant-or-self::*[@id = 'pageContainer']\" data='<div id=\"pageContainer\">\\r\\n      <header '>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.css('#pageContainer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.css('.eventspage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Selector xpath='//*[@id=\"pageContainer\"]/main/div[3]/p[3]' data='<p>\\r\\n            <strong>Allegheny Count'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rxp = response.xpath('//*[@id=\"pageContainer\"]/main/div[3]/p[3]')[0]\n",
    "# len( rxp )\n",
    "rxp"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "selector from chromedev tools\n",
    "\n",
    "#pageContainer > main > div.contentWrapper.htmlContent > p:nth-child(7)\n",
    "\n",
    "XPath:\n",
    "    //*[@id=\"pageContainer\"]/main/div[3]/p[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector xpath='descendant-or-self::strong' data='<strong>Allegheny County Airport Authori'>,\n",
       " <Selector xpath='descendant-or-self::strong' data='<strong>\\r\\n              <u>2019 Board Me'>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rxp.css('strong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utag = rxp.css('u')\n",
    "uYearHeader = utag.extract_first()\n",
    "uYearHeader[3:7]\n",
    "#utag.extract_unquoted()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rxp.css('br').getall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mrxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m        Apply the given CSS selector and return a :class:`SelectorList` instance.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        ``query`` is a string containing the CSS selector to apply.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        In the background, CSS queries are translated into XPath queries using\u001b[0m\n",
       "\u001b[0;34m        `cssselect`_ library and run ``.xpath()`` method.\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_css2xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.6/site-packages/parsel/selector.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rxp.css??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>\\r\\n            <strong>Allegheny County Airport Authority 2019 Board Meetings</strong><br>\\r\\n            <em>*Board Meetings will be held\\xa0on the 3rd Friday of the month at 11:30 a.m. in Conference Room A, 4th Flr Mezzanine, Landside Terminal, Pittsburgh International Airport, unless otherwise noted below.</em><br><br>\\r\\n            <strong>\\r\\n              <u>2019 Board Meeting Dates</u><br>\\r\\nJanuary 18<br>\\r\\nFebruary 15<br>\\r\\nMarch 15\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<br>\\r\\nApril 26<br>\\r\\nMay 17\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<br>\\r\\nJune 21\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<br>\\r\\nJuly 19<br>\\r\\n*August – NO BOARD MEETING<br>\\r\\n*September 20 – (Allegheny County Airport, West Mifflin)<br>\\r\\nOctober 18<br>\\r\\nNovember 15<br>\\r\\nDecember 20</strong><br><br>\\r\\nFor information, call (412) 472-3500.</p>'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = rxp.extract()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<p>\\r',\n",
       " '            <strong>Allegheny County Airport Authority 2019 Board Meetings</strong><br>\\r']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = text.split(\"\\n\")\n",
    "# pop is from end\n",
    "# lines.pop()\n",
    "lines[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<p>\\r',\n",
       " '            <strong>Allegheny County Airport Authority 2019 Board Meetings</strong><br>\\r',\n",
       " '            <em>*Board Meetings will be held\\xa0on the 3rd Friday of the month at 11:30 a.m. in Conference Room A, 4th Flr Mezzanine, Landside Terminal, Pittsburgh International Airport, unless otherwise noted below.</em><br><br>\\r',\n",
       " '            <strong>\\r',\n",
       " '              <u>2019 Board Meeting Dates</u><br>\\r',\n",
       " 'January 18<br>\\r',\n",
       " 'February 15<br>\\r',\n",
       " 'March 15\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<br>\\r',\n",
       " 'April 26<br>\\r',\n",
       " 'May 17\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<br>\\r',\n",
       " 'June 21\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<br>\\r',\n",
       " 'July 19<br>\\r',\n",
       " '*August – NO BOARD MEETING<br>\\r',\n",
       " '*September 20 – (Allegheny County Airport, West Mifflin)<br>\\r',\n",
       " 'October 18<br>\\r',\n",
       " 'November 15<br>\\r',\n",
       " 'December 20</strong><br><br>\\r',\n",
       " 'For information, call (412) 472-3500.</p>']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n",
      "root      1070  0.0  0.0  20288   872 ?        Ss+  00:18   0:00 bash\n",
      "root         1  0.0  0.0   4516    24 ?        Ss    2018   7:39 tini -g -- start-notebook.sh\n",
      "root         8  0.0  0.0  51492     8 ?        S     2018   0:00 sudo -E -H -u jovyan PATH=/opt/conda/bin:/usr/local/sbin:/usr/local\n",
      "jovyan      33  0.0  0.2 902212 31168 ?        Sl    2018 130:52  \\_ /opt/conda/bin/python /opt/conda/bin/jupyter-lab\n",
      "jovyan      48  0.0  0.0  20308   872 pts/0    Ss+   2018   0:00      \\_ /bin/bash\n",
      "jovyan      70  0.0  0.0 1185984    0 ?        Ssl   2018   9:26      \\_ /opt/conda/bin/python -m ipykernel_launcher -f /home/jovyan\n",
      "jovyan     176  0.0  0.0 991024     0 ?        Ssl  Jan03   9:11      \\_ /opt/conda/bin/python -m ipykernel_launcher -f /home/jovyan\n",
      "jovyan     544  0.0  0.0 644484  8052 ?        Ssl  Jan04   8:41      \\_ /opt/conda/bin/python -m ipykernel_launcher -f /home/jovyan\n",
      "jovyan     561  0.0  0.0 1175316 8092 ?        Ssl  Jan04   8:48      \\_ /opt/conda/bin/python -m ipykernel_launcher -f /home/jovyan\n",
      "jovyan     695  0.0  0.0 1300228    0 ?        Ssl  Jan08   8:35      \\_ /opt/conda/bin/python -m ipykernel_launcher -f /home/jovyan\n",
      "jovyan     867  0.0  0.1 638596 19520 ?        Ssl  Mar08   2:23      \\_ /opt/conda/bin/python -m ipykernel_launcher -f /home/jovyan\n",
      "jovyan     898  0.0  0.8 1192484 108048 ?      Ssl  Mar08   2:31      \\_ /opt/conda/bin/python -m ipykernel_launcher -f /home/jovyan\n",
      "jovyan    1261  0.0  0.0   4624   832 pts/1    Ss+  17:50   0:00          \\_ /bin/sh -c ps auxwf\n",
      "jovyan    1262  0.0  0.0  36068  3312 pts/1    R+   17:50   0:00              \\_ ps auxwf\n"
     ]
    }
   ],
   "source": [
    "!ps auxwf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n",
      "root      1070  0.0  0.0  20288   872 ?        Ss+  00:18   0:00 bash\n",
      "root         1  0.0  0.0   4516    24 ?        Ss    2018   7:39 tini -g -- start-notebook.sh\n",
      "root         8  0.0  0.0  51492     8 ?        S     2018   0:00 sudo -E -H -u jovyan PATH=/opt/conda/bin:/usr/local/sbin:/usr/local\n",
      "jovyan      33  0.0  0.2 902212 32252 ?        Sl    2018 130:55  \\_ /opt/conda/bin/python /opt/conda/bin/jupyter-lab\n",
      "jovyan      48  0.0  0.0  20308   872 pts/0    Ss+   2018   0:00      \\_ /bin/bash\n",
      "jovyan      70  0.0  0.0 1185984    0 ?        Ssl   2018   9:26      \\_ /opt/conda/bin/python -m ipykernel_launcher -f /home/jovyan\n",
      "jovyan     176  0.0  0.0 991024     0 ?        Ssl  Jan03   9:11      \\_ /opt/conda/bin/python -m ipykernel_launcher -f /home/jovyan\n",
      "jovyan     544  0.0  0.0 644484  8052 ?        Ssl  Jan04   8:41      \\_ /opt/conda/bin/python -m ipykernel_launcher -f /home/jovyan\n",
      "jovyan     561  0.0  0.0 1175316 8092 ?        Ssl  Jan04   8:48      \\_ /opt/conda/bin/python -m ipykernel_launcher -f /home/jovyan\n",
      "jovyan     695  0.0  0.0 1300228    0 ?        Ssl  Jan08   8:35      \\_ /opt/conda/bin/python -m ipykernel_launcher -f /home/jovyan\n",
      "jovyan     898  0.0  0.8 1192484 108060 ?      Ssl  Mar08   2:31      \\_ /opt/conda/bin/python -m ipykernel_launcher -f /home/jovyan\n",
      "jovyan    1305  0.0  0.0   4624   784 pts/1    Ss+  18:24   0:00      |   \\_ /bin/sh -c ps auxwf\n",
      "jovyan    1306  0.0  0.0  36068  3368 pts/1    R+   18:24   0:00      |       \\_ ps auxwf\n",
      "jovyan    1267  0.1  0.3 637936 44536 ?        Ssl  18:15   0:00      \\_ /opt/conda/bin/python -m ipykernel_launcher -f /home/jovyan\n"
     ]
    }
   ],
   "source": [
    "!ps auxwf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo hey wake up >> /dev/pts/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Write tests\n",
    "Our general approach to writing tests is to save a copy of a site’s HTML in tests/files and then use that HTML to verify the behavior of each spider. In this way, we avoid needing a network connection to run tests and our tests don’t break every time a site’s content is updated.\n",
    "\n",
    "Here is the test setup and an example test:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Create a Pull Request\n",
    "If your ready to submit your code to the project, you should create a pull request on GitHub. You can do this as early as you would like in order to get feedback from others working on the project. In this case, please prefix your pull request name with WIP so that everyone knows what kind of feedback you are looking for.\n",
    "\n",
    "Additionally, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
